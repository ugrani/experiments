{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPkXpOQHupaXwcC5SY75G/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ugrani/experiments/blob/main/rnr_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaZYsyfY_PzK"
      },
      "outputs": [],
      "source": [
        "#get the dataset from hugging face\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"ms_marco\",\n",
        "    \"v1.1\",\n",
        "    split=\"validation\"\n",
        ")\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "oWijh5XtAhrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[0]\n",
        "\n",
        "print(\"Query:\")\n",
        "print(example[\"query\"])\n",
        "\n",
        "print(\"\\nPassages (showing first 2):\")\n",
        "for p in example[\"passages\"][\"passage_text\"]:\n",
        "  print(\"-\", p[:200], \"...\")"
      ],
      "metadata": {
        "id": "j4AsHLjwAwpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subsample aggressively to control the GPU cost\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "NUM_QUERIES = 200\n",
        "TARGET_PASSAGES = 10_000\n",
        "NUM_PASSAGES = 10_000\n",
        "\n",
        "sampled_queries = random.sample(range(len(dataset)), NUM_QUERIES)\n",
        "\n",
        "queries = []\n",
        "relevant_passages = []\n",
        "all_passages = []\n",
        "\n",
        "for idx in sampled_queries:\n",
        "    row = dataset[idx]\n",
        "    queries.append(row[\"query\"])\n",
        "\n",
        "    for passage in row[\"passages\"][\"passage_text\"]:\n",
        "        all_passages.append(passage)\n",
        "\n",
        "# Deduplicate and subsample passages\n",
        "all_passages = list(set(all_passages))\n",
        "all_passages = random.sample(\n",
        "    all_passages,\n",
        "    min(NUM_PASSAGES, len(all_passages))\n",
        ")\n",
        "\n",
        "print(f\"Queries: {len(queries)}\")\n",
        "print(f\"Passages: {len(all_passages)}\")\n",
        "\n",
        "# Start corpus with ALL passages from your sampled queries (so relevant ones are included)\n",
        "corpus = set()\n",
        "for idx in sampled_queries:\n",
        "    row = dataset[idx]\n",
        "    for p in row[\"passages\"][\"passage_text\"]:\n",
        "        corpus.add(p)\n",
        "\n",
        "print(\"Initial corpus size (from sampled queries):\", len(corpus))\n",
        "\n",
        "# Add distractor passages from OTHER queries in validation split\n",
        "all_indices = list(range(len(dataset)))\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "for idx in all_indices:\n",
        "    if idx in sampled_queries:\n",
        "        continue\n",
        "    row = dataset[idx]\n",
        "    for p in row[\"passages\"][\"passage_text\"]:\n",
        "        corpus.add(p)\n",
        "        if len(corpus) >= TARGET_PASSAGES:\n",
        "            break\n",
        "    if len(corpus) >= TARGET_PASSAGES:\n",
        "        break\n",
        "\n",
        "all_passages = list(corpus)\n",
        "print(\"Final corpus size:\", len(all_passages))\n"
      ],
      "metadata": {
        "id": "y1A2bXulGZLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the relevance\n",
        "query_to_relevant = {}\n",
        "\n",
        "for idx in sampled_queries:\n",
        "    row = dataset[idx]\n",
        "    query = row[\"query\"]\n",
        "\n",
        "    relevant = [\n",
        "        p for p, is_rel in zip(\n",
        "            row[\"passages\"][\"passage_text\"],\n",
        "            row[\"passages\"][\"is_selected\"]\n",
        "        )\n",
        "        if is_rel == 1\n",
        "    ]\n",
        "\n",
        "    query_to_relevant[query] = relevant\n"
      ],
      "metadata": {
        "id": "mFRm5e4mHO9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "96JjrdrDHlYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Encode passages\n",
        "passage_emb = model.encode(\n",
        "    all_passages,\n",
        "    batch_size=128,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,   # important for cosine via dot product\n",
        ")\n",
        "\n",
        "print(passage_emb.shape)  # (num_passages, dim)\n"
      ],
      "metadata": {
        "id": "UT5r-VlwHrU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "dim = passage_emb.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)  # inner product; works with normalized embeddings as cosine\n",
        "index.add(passage_emb)\n",
        "\n",
        "print(\"FAISS ntotal:\", index.ntotal)\n"
      ],
      "metadata": {
        "id": "-A991oYFJSBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=50):\n",
        "    q_emb = model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    scores, idxs = index.search(q_emb, k)\n",
        "    return [all_passages[i] for i in idxs[0]], scores[0]\n"
      ],
      "metadata": {
        "id": "CmeHR4KWJ_1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = queries[0]\n",
        "top, scores = retrieve(q, k=5)\n",
        "print(\"Query:\", q)\n",
        "print(\"\\nTop results:\")\n",
        "for i, p in enumerate(top, 1):\n",
        "    print(f\"\\n#{i} (score={scores[i-1]:.3f})\\n{p[:300]}...\")\n"
      ],
      "metadata": {
        "id": "nEV-PO7BKCW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(queries, query_to_relevant, k):\n",
        "    hits = 0\n",
        "    eligible = 0\n",
        "\n",
        "    for q in queries:\n",
        "        rels = query_to_relevant.get(q, [])\n",
        "        if not rels:   # some queries may have 0 labeled relevant passages\n",
        "            continue\n",
        "\n",
        "        eligible += 1\n",
        "        retrieved, _ = retrieve(q, k=k)\n",
        "        retrieved_set = set(retrieved)\n",
        "\n",
        "        # hit if ANY relevant passage appears in top-k\n",
        "        if any(r in retrieved_set for r in rels):\n",
        "            hits += 1\n",
        "\n",
        "    return hits / eligible if eligible else 0.0, eligible\n",
        "\n",
        "Ks = [10, 50, 100, 200]\n",
        "for k in Ks:\n",
        "    r, n = recall_at_k(queries, query_to_relevant, k)\n",
        "    print(f\"Recall@{k}: {r:.3f}  (evaluated on {n} queries)\")\n"
      ],
      "metadata": {
        "id": "SnnJILx_KNvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "recalls = []\n",
        "for k in Ks:\n",
        "    r, _ = recall_at_k(queries, query_to_relevant, k)\n",
        "    recalls.append(r)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Ks, recalls, marker=\"o\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Recall@K\")\n",
        "plt.title(\"Stage-1 Dense Retrieval Recall vs K\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GiiMNp07KWeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#next phas focusses on\n",
        "\n",
        "\n",
        "Dense retriever gets top-K candidates\n",
        "\n",
        "Cross-encoder scores (query, passage) pairs\n",
        "\n",
        "Reorder candidates\n",
        "\n",
        "Evaluate MRR@10 and NDCG@10 using your is_selected labels"
      ],
      "metadata": {
        "id": "fuk_lm9KMJo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "hvEpGDAWMMLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load a strong MS MARCO cross-encoder\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "reranker_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(reranker_name)\n"
      ],
      "metadata": {
        "id": "JtWvIm4eMOF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a query→relevant set in passage-text space\n",
        "query_to_relevant_set = {q: set(rels) for q, rels in query_to_relevant.items()}\n"
      ],
      "metadata": {
        "id": "abnbPM2EMX7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rerank function\n",
        "import numpy as np\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    pairs = [(query, p) for p in candidates]\n",
        "    scores = reranker.predict(pairs)  # numpy array\n",
        "    order = np.argsort(-scores)       # descending\n",
        "    reranked = [candidates[i] for i in order]\n",
        "    reranked_scores = scores[order]\n",
        "    return reranked, reranked_scores\n"
      ],
      "metadata": {
        "id": "onTFHWwVMcak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Metrics: MRR@10 and NDCG@10\n",
        "import math\n",
        "\n",
        "def mrr_at_k(ranked_list, relevant_set, k=10):\n",
        "    for i, p in enumerate(ranked_list[:k], start=1):\n",
        "        if p in relevant_set:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(ranked_list, relevant_set, k=10):\n",
        "    dcg = 0.0\n",
        "    for i, p in enumerate(ranked_list[:k], start=1):\n",
        "        rel = 1.0 if p in relevant_set else 0.0\n",
        "        if rel > 0:\n",
        "            dcg += rel / math.log2(i + 1)\n",
        "    # Ideal DCG with binary rels: best case is 1 relevant at rank 1 (for our “any rel” framing)\n",
        "    idcg = 1.0\n",
        "    return dcg / idcg\n"
      ],
      "metadata": {
        "id": "DT-w38sXMgtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate: dense-only vs dense+rerank\n",
        "def eval_ranking(queries, query_to_relevant_set, K_retrieve=50, K_eval=10, do_rerank=False):\n",
        "    mrrs, ndcgs = [], []\n",
        "    eligible = 0\n",
        "\n",
        "    for q in queries:\n",
        "        rels = query_to_relevant_set.get(q, set())\n",
        "        if not rels:\n",
        "            continue\n",
        "        eligible += 1\n",
        "\n",
        "        candidates, _ = retrieve(q, k=K_retrieve)  # dense top-K\n",
        "\n",
        "        ranked = candidates\n",
        "        if do_rerank:\n",
        "            ranked, _ = rerank(q, candidates)\n",
        "\n",
        "        mrrs.append(mrr_at_k(ranked, rels, k=K_eval))\n",
        "        ndcgs.append(ndcg_at_k(ranked, rels, k=K_eval))\n",
        "\n",
        "    return float(np.mean(mrrs)), float(np.mean(ndcgs)), eligible\n",
        "\n",
        "K_retrieve = 50\n",
        "mrr_dense, ndcg_dense, n = eval_ranking(queries, query_to_relevant_set, K_retrieve=K_retrieve, do_rerank=False)\n",
        "mrr_rerank, ndcg_rerank, _ = eval_ranking(queries, query_to_relevant_set, K_retrieve=K_retrieve, do_rerank=True)\n",
        "\n",
        "print(f\"Evaluated on {n} queries, retrieve K={K_retrieve}\")\n",
        "print(f\"Dense only   -> MRR@10: {mrr_dense:.4f}, NDCG@10: {ndcg_dense:.4f}\")\n",
        "print(f\"+ Reranker   -> MRR@10: {mrr_rerank:.4f}, NDCG@10: {ndcg_rerank:.4f}\")\n"
      ],
      "metadata": {
        "id": "-wY7zkWbM71H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how K impacts reranker cost/quality\n",
        "\n",
        "for K in [10, 20, 50, 100]:\n",
        "    mrr_dense, ndcg_dense, n = eval_ranking(queries, query_to_relevant_set, K_retrieve=K, do_rerank=False)\n",
        "    mrr_rerank, ndcg_rerank, _ = eval_ranking(queries, query_to_relevant_set, K_retrieve=K, do_rerank=True)\n",
        "    print(f\"K={K:>3} | Dense MRR@10 {mrr_dense:.4f} NDCG@10 {ndcg_dense:.4f} \"\n",
        "          f\"|| Rerank MRR@10 {mrr_rerank:.4f} NDCG@10 {ndcg_rerank:.4f}\")"
      ],
      "metadata": {
        "id": "c_Ood8LlbhXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Ks = [10, 20, 50, 100]\n",
        "\n",
        "dense_mrr = [0.5643, 0.5643, 0.5643, 0.5643]\n",
        "rerank_mrr = [0.6640, 0.6649, 0.6655, 0.6655]\n",
        "\n",
        "dense_ndcg = [0.6890, 0.6890, 0.6890, 0.6890]\n",
        "rerank_ndcg = [0.7699, 0.7727, 0.7743, 0.7743]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Ks, dense_mrr, marker=\"o\", label=\"Dense only\")\n",
        "plt.plot(Ks, rerank_mrr, marker=\"o\", label=\"Dense + Cross-Encoder\")\n",
        "\n",
        "plt.xlabel(\"K (Candidates Retrieved)\")\n",
        "plt.ylabel(\"MRR@10\")\n",
        "plt.title(\"MRR@10 vs Candidate Set Size (K)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Ks, dense_ndcg, marker=\"o\", label=\"Dense only\")\n",
        "plt.plot(Ks, rerank_ndcg, marker=\"o\", label=\"Dense + Cross-Encoder\")\n",
        "\n",
        "plt.xlabel(\"K (Candidates Retrieved)\")\n",
        "plt.ylabel(\"NDCG@10\")\n",
        "plt.title(\"NDCG@10 vs Candidate Set Size (K)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xTQEGqXHd_ki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}